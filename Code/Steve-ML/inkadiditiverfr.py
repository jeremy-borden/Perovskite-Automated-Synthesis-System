# -*- coding: utf-8 -*-
"""InkAdiditiveRFR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G9doAoSJyh1afXpvzvpmOe8yBTKKe1kY

# **1. Importing Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from google.colab import files
!pip install scikit-optimize
from skopt import gp_minimize
from skopt.space import Integer, Real
from skopt.utils import use_named_args
import warnings
import pickle
from sklearn.model_selection import GridSearchCV
from skopt.space import Real, Categorical
import scipy.stats as stats
import shap

"""# **2. Uploading and Loading Data**"""

# Upload the file
uploaded = files.upload()
data_file = list(uploaded.keys())[0]
data = pd.read_excel(data_file)

"""# **3. Function to display all the features in the target dataset**"""

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
display(data.style.hide(axis="index"))

"""# **4. Data Preprocessing & Splitting (Feature Encoding, Input-Output Setup)**

"""

# Calculate Bandgap
data['Bandgap'] = 1240 / data['Wavelength'].replace(0, np.nan)

# Handle infinite values in Bandgap
data['Bandgap'] = data['Bandgap'].replace([np.inf, -np.inf], np.nan)
data.dropna(subset=['Bandgap'], inplace=True)

# Define input features (Intensity, Ink, Additive) and output (Bandgap)
X = data[['Intensity', 'Ink', 'Additive']]
y = data['Bandgap']

# One-Hot Encode categorical variables
X = pd.get_dummies(X, columns=['Ink', 'Additive'], dtype=bool)
sample_no = data['Sample_No']

# split into training and testing sets
X_train, X_test, y_train, y_test, sample_no_train, sample_no_test = train_test_split(
    X, y, sample_no, test_size=0.2, random_state=42
)

# Display the shapes of the training and testing sets
print("Training data shape:", X_train.shape, y_train.shape)
print("Testing data shape:", X_test.shape, y_test.shape)

"""# **5. Organizing & Displaying Processed Data**"""

#train data
train_data = X_train.copy()
train_data['Bandgap'] = y_train
train_data['Sample_No'] = sample_no_train
train_data.set_index('Sample_No', inplace=True)

#test data
test_data = X_test.copy()
test_data['Bandgap'] = y_test
test_data['Sample_No'] = sample_no_test
test_data.set_index('Sample_No', inplace=True)

# display training data
print("\n Full Training Data :")
print(train_data.reset_index(drop=False).to_markdown(index=False))

# display testing data
print("\n Full Testing Data :")
print(test_data.reset_index(drop=False).to_markdown(index=False))

"""# **6. Model Training and Making Predictions**"""

# Initialize and train the Random Forest Regressor model
rfr_model = RandomForestRegressor(n_estimators=100, random_state=42)
rfr_model.fit(X_train, y_train)
y_pred = rfr_model.predict(X_test)
print("Random Forest model successfully trained")

"""# **7. Model Evaluations**

### **7.1 Initial model evaluation before tuning**
"""

# Evaluate initial model before tuning
initial_mae = mean_absolute_error(y_test, y_pred)
initial_r2 = r2_score(y_test, y_pred)
print(f"Initial Model - MAE: {initial_mae}, R² Score: {initial_r2}")

"""### **7.2 Hyperparameter Tuning using Grid SearchCV**



"""

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rfr_model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R² Score: {r2}")

"""### **7.3. Hyperparameter Optimization Useing Bayesian Optimization**

**Hyperparameter Optimization for Random Forest Uses Bayesian Optimization to find the best hyperparameters for a Random Forest model by minimizing the negative R² score. Optimized parameters are used to train the final Random Forest model.**
"""

warnings.filterwarnings("ignore", category=UserWarning)

# Define the refined hyperparameter space
param_space = [
    Integer(100, 300, name="n_estimators"),
    Integer(10, 50, name="max_depth"),
    Integer(5, 15, name="min_samples_split"),
    Integer(2, 6, name="min_samples_leaf")
]

@use_named_args(param_space)
def objective(**params):
    model = RandomForestRegressor(
        n_estimators=params["n_estimators"],
        max_depth=params["max_depth"],
        min_samples_split=params["min_samples_split"],
        min_samples_leaf=params["min_samples_leaf"],
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return -r2_score(y_test, y_pred)  # Minimize negative R²

# Run Bayesian Optimization with refined space
res = gp_minimize(objective, param_space, n_calls=20, random_state=42)

# Extract best parameters
best_params = {
    "n_estimators": res.x[0],
    "max_depth": res.x[1],
    "min_samples_split": res.x[2],
    "min_samples_leaf": res.x[3]
}
print("Optimized Parameters:", best_params)

# Train model with optimized parameters
optimized_model = RandomForestRegressor(**best_params, random_state=42)
optimized_model.fit(X_train, y_train)

# Make predictions
y_pred_opt = optimized_model.predict(X_test)

# Evaluate the model
mae_opt = mean_absolute_error(y_test, y_pred_opt)
r2_opt = r2_score(y_test, y_pred_opt)

print(f"Optimized Model - MAE: {mae_opt}, R² Score: {r2_opt}")

"""# **8. Visualization Functions**

### **8.1. Visualization comparing R² Score and MAE across all three models:**
"""

# Define models and their respective performance metrics
models = ['Initial Model', 'Tuned Model', 'Optimized Model']
mae_values = [initial_mae, mae, mae_opt]
r2_values = [initial_r2, r2, r2_opt]

# Plot MAE comparison
plt.figure(figsize=(8, 4))
plt.bar(models, mae_values, color=['red', 'blue', 'green'])
plt.ylabel('Mean Absolute Error (MAE)')
plt.title('MAE Comparison: Initial vs. Tuned vs. Optimized Model')
plt.show()

# Plot R² Score comparison
plt.figure(figsize=(8, 4))
plt.bar(models, r2_values, color=['red', 'blue', 'green'])
plt.ylabel('R² Score')
plt.title('R² Score Comparison: Initial vs. Tuned vs. Optimized Model')
plt.show()

"""### **8.2.  Model coefficients and Plot for feature importances**

**To display the importance of each feature in predicting the Bandgap.**

**Helps prioritize variables for feature engineering or model refinement.**
"""

feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rfr_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'], color='skyblue')
plt.xlabel('Feature Importance')
plt.title('Feature Importances from Random Forest Regressor')
plt.gca().invert_yaxis()
plt.grid(axis='x')
plt.show()

# Feature Importance
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rfr_model.feature_importances_
})
print("\nFeature Importances:")
print(feature_importances)

# SHAP plot
explainer = shap.TreeExplainer(rfr_model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)

"""**A strong, positive relationship between PL intensity and bandgap means stable and efficient bandgap.**

### **8.3 Scatter Plot of Actual vs Predicted Values**
"""

# all data points
y_pred_all = rfr_model.predict(X)

plt.figure(figsize=(10, 6))
plt.scatter(y, y_pred_all, alpha=0.6, edgecolor=None, c='blue', label='All Data')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Prediction Line')
plt.xlabel('Actual Bandgap')
plt.ylabel('Predicted Bandgap')
plt.title('Actual vs Predicted Bandgap (Random Forest)')
plt.legend()
plt.grid()
plt.show()

"""**The closer the points are to the line, the better the model’s performance. Wider spread indicates poor predictive accuracy.**

### **8.4 Residual Plot vs Predicted Values**
"""

# all data points
y_pred_all = rfr_model.predict(X)

# Residuals: all data points
residuals_all = y - y_pred_all

plt.figure(figsize=(10, 6))
plt.scatter(y_pred_all, residuals_all, alpha=0.6, edgecolor=None, c='green', label='All Data')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.xlabel('Predicted Bandgap')
plt.ylabel('Residuals')
plt.title('Residuals vs Predicted Bandgap (All Data)')
plt.legend()
plt.grid()
plt.show()

"""**Indicates that the model is unbiased and captures the data well. Ideally, residuals should cluster close to zero.**

**(residuals = actual - predicted)**

### **8.5 Density Distribution Plot of Residuals**
"""

# Calculate residuals
y_pred_all = rfr_model.predict(X)
residuals_all = y - y_pred_all

# Plot
plt.figure(figsize=(10, 6))
sns.kdeplot(residuals_all, fill=True, color='blue', alpha=0.6, label='Residual Density')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Density Distribution of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Density')
plt.legend()
plt.grid()
plt.show()

"""**The density plot is symmetrical and centered at 0 and spread is minimal, the model is performing well across the dataset**

# **9. Efficiency Calculation and Visualization**
"""

def calculate_efficiency(bandgap):
    E_loss = 0.3  # Energy loss (approximate)
    E_max = 2.0   # Maximum theoretical efficiency factor, shockley-queisser limit
    efficiency = (bandgap - E_loss) / E_max
    efficiency = np.clip(efficiency, 0, 1)
    return efficiency

# Calculate efficiency
data['Efficiency'] = calculate_efficiency(data['Bandgap'])

# Print efficiency scores
print(data[['Bandgap', 'Efficiency']])

# Efficiency Distribution Plot
plt.figure(figsize=(8, 6))
sns.histplot(data['Efficiency'], bins=30, kde=True, color="green")
plt.xlabel("Efficiency")
plt.ylabel("Count")
plt.title("Efficiency Distribution")
plt.grid(True)
plt.show()

"""**If the peak is around 0.6, most samples have 60% efficiency.**

# **10. Finding the Most Efficient Sample in the Dataset**
"""

most_efficient_sample = data.loc[data['Efficiency'].idxmax()]
most_efficient_sample_df = most_efficient_sample.to_frame().T

# Print
print("\nMost Efficient Sample in the Dataset:")
print(most_efficient_sample_df.to_string(index=False))

top_samples = data.nlargest(3, 'Efficiency')

# Plot Wavelength vs Efficiency
plt.figure(figsize=(8, 5))
plt.scatter(data["Wavelength"], data["Efficiency"], label="All Samples", alpha=0.7, color='blue')
plt.scatter(top_samples["Wavelength"], top_samples["Efficiency"],
            color=['red', 'green', 'purple'], label="Top Efficient Samples", s=100, edgecolors='black')

z = np.polyfit(data["Wavelength"], data["Efficiency"], 1)
p = np.poly1d(z)
plt.plot(data["Wavelength"], p(data["Wavelength"]), "r--", label="Trend Line")
plt.annotate(f"Max: {most_efficient_sample['Efficiency']:.2f}",
             (most_efficient_sample["Wavelength"], most_efficient_sample["Efficiency"]),
             textcoords="offset points", xytext=(-20,10), ha='center', fontsize=10, color='red')

plt.xlabel("Wavelength (nm)")
plt.ylabel("Efficiency")
plt.title("Most Efficient Sample in the Dataset")
plt.legend()
plt.grid(True)
plt.show()

# KDE Plot of Intensity vs Efficiency
plt.figure(figsize=(10, 6))
sns.kdeplot(
    x=data['Intensity'],
    y=data['Efficiency'],
    cmap="coolwarm",
    fill=True,
    thresh=0.05,
    bw_adjust=0.8,
    cbar=True
)
plt.xlabel("Intensity")
plt.ylabel("Efficiency")
plt.title("Density Distribution of Intensity vs Efficiency")
plt.show()

"""# **12. Optimization Loop beyond the dataset using Random Search**

**Model performs a random search by generating new, unseen combinations of Intensity, Ink, and Additive beyond the original dataset. It uses the trained Random Forest model to predict Bandgap for these combinations and identifies the optimal parameters that maximize Bandgap. This allows for the exploration of a wider parameter space to find better configurations.**
"""

num_samples = 1000  # Can be adjusted

# Generate random parameter samples
intensity_samples = np.random.uniform(500, 5000, num_samples)  # Intensity range
ink_samples = np.random.choice(['FASnI3', 'MASnI3', 'mix'], num_samples)
additive_samples = np.random.choice(['0', '4-MePEABr', 'Br', 'EASCN', 'MA', 'Zn'], num_samples)
random_samples = pd.DataFrame({
    'Intensity': intensity_samples,
    'Ink': ink_samples,
    'Additive': additive_samples
})

# One-Hot Encode categorical variables
random_samples = pd.get_dummies(random_samples, columns=['Ink', 'Additive'], dtype=bool)
random_samples = random_samples.reindex(columns=X_train.columns, fill_value=False)

# Predict Bandgap & Wavelength using our trained Random Forest model
random_samples['Bandgap'] = rfr_model.predict(random_samples)
random_samples['Wavelength'] = 1240 / random_samples['Bandgap']

# Find the best parameters
best_sample = random_samples.loc[random_samples['Bandgap'].idxmax()]
optimal_intensity = best_sample['Intensity']
optimal_ink = best_sample.filter(like="Ink_").idxmax().replace("Ink_", "")
optimal_additive = best_sample.filter(like="Additive_").idxmax().replace("Additive_", "")
optimal_bandgap = best_sample['Bandgap']
optimal_wavelength = best_sample['Wavelength']

# Save results to CSV
random_samples.to_csv("random_search_results.csv", index=False)

# Plot
plt.figure(figsize=(10, 6))
sc = plt.scatter(
    random_samples['Intensity'],
    random_samples['Wavelength'],
    c=random_samples['Bandgap'], cmap='viridis', s=50, alpha=0.7, edgecolors='k'
)
plt.colorbar(sc, label='Bandgap')
plt.title('Random Search: Wavelength vs. Intensity (Bandgap)', fontsize=16)
plt.xlabel('Intensity', fontsize=14)
plt.ylabel('Wavelength (nm)', fontsize=14)
plt.grid(True)
plt.savefig("random_search_plot.png")
plt.show()

# Print best parameters
print("\n Optimal Parameters Found (Random Search):")
print(f"# Intensity: {optimal_intensity:.2f}")
print(f"# Ink: {optimal_ink}")
print(f"# Additive: {optimal_additive}")
print(f"# Maximum Bandgap: {optimal_bandgap:.4f}")
print(f"# Corresponding Wavelength: {optimal_wavelength:.2f} nm")

"""# **13. Optimization Loop beyond the dataset using Bayesian Optimization**

**Model uses Bayesian Optimization to efficiently explore new, unseen combinations of Intensity, Ink, and Additive beyond the dataset. It iteratively suggests parameter combinations, predicts Bandgap using the trained model, and identifies the optimal values that maximize Bandgap. This method is more systematic and efficient than random search, focusing on promising regions of the parameter space.**
"""

def objective(x):
    """Objective function to maximize bandgap (minimize negative bandgap)."""

    intensity, ink, additive = x

    # Create DataFrame for input parameters
    input_data = pd.DataFrame({
        'Intensity': [intensity],
        'Ink': [ink],
        'Additive': [additive]
    })

    # One-Hot Encode categorical variables
    input_data = pd.get_dummies(input_data, columns=['Ink', 'Additive'], dtype=bool)
    input_data = input_data.reindex(columns=X_train.columns, fill_value=False)

    # Predict Bandgap using the trained Random Forest model
    bandgap = rfr_model.predict(input_data)[0]

    return -bandgap  # Minimize negative Bandgap (since we want to maximize it)

# Define the search space
search_space = [
    Real(500, 5000, name='Intensity'),
    Categorical(['FASnI3', 'MASnI3', 'mix'], name='Ink'),
    Categorical(['0', '4-MePEABr', 'Br', 'EASCN', 'MA', 'Zn'], name='Additive')
]

# Perform Bayesian Optimization
res = gp_minimize(objective, search_space, n_calls=20, random_state=42)

# best parameters
optimal_intensity, optimal_ink, optimal_additive = res.x
optimal_bandgap = -res.fun
optimal_wavelength = 1240 / optimal_bandgap

# Print best parameters
print("\n Optimal Parameters Found (Bayesian Optimization):")
print(f" Intensity: {optimal_intensity:.2f}")
print(f" Ink: {optimal_ink}")
print(f" Additive: {optimal_additive}")
print(f" Maximum Bandgap: {optimal_bandgap:.4f}")
print(f" Corresponding Wavelength: {optimal_wavelength:.2f} nm")

# Plot
plt.figure(figsize=(8, 5))
plt.plot(res.func_vals, marker='o', linestyle='-', label='Objective Function Value')
plt.axhline(y=-optimal_bandgap, color='r', linestyle='--', label='Optimal Bandgap')
plt.xlabel('Iteration', fontsize=14)
plt.ylabel('Negative Bandgap Score', fontsize=14)
plt.title('Bayesian Optimization Convergence', fontsize=16)
plt.legend()
plt.grid()
plt.savefig("bayesian_optimization_convergence.png")
plt.show()

# Save the trained model using pickle
with open('random_forest_model.pkl', 'wb') as file:
    pickle.dump(rfr_model, file)

# Download the saved model to your local machine
files.download('random_forest_model.pkl')